{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d27f40",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64570899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathway\n",
      "  Downloading pathway-0.post1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.3.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Downloading numpy-2.4.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\tahmi\\documents\\work\\hackathons\\iitk_26\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tahmi\\documents\\work\\hackathons\\iitk_26\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Downloading pathway-0.post1-py3-none-any.whl (2.8 kB)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading faiss_cpu-1.13.2-cp312-cp312-win_amd64.whl (18.9 MB)\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/18.9 MB 1.5 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.3/18.9 MB 1.8 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.6/18.9 MB 146.9 kB/s eta 0:01:58\n",
      "   --- ------------------------------------ 1.8/18.9 MB 171.8 kB/s eta 0:01:40\n",
      "   ---- ----------------------------------- 2.1/18.9 MB 196.4 kB/s eta 0:01:26\n",
      "   ---- ----------------------------------- 2.4/18.9 MB 220.8 kB/s eta 0:01:15\n",
      "   ----- ---------------------------------- 2.6/18.9 MB 245.1 kB/s eta 0:01:07\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 2.9/18.9 MB 266.7 kB/s eta 0:01:01\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------ --------------------------------- 3.1/18.9 MB 232.7 kB/s eta 0:01:08\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.4/18.9 MB 219.8 kB/s eta 0:01:11\n",
      "   ------- -------------------------------- 3.7/18.9 MB 205.4 kB/s eta 0:01:15\n",
      "   ------- -------------------------------- 3.7/18.9 MB 205.4 kB/s eta 0:01:15\n",
      "   ------- -------------------------------- 3.7/18.9 MB 205.4 kB/s eta 0:01:15\n",
      "   -------- ------------------------------- 3.9/18.9 MB 213.1 kB/s eta 0:01:11\n",
      "   -------- ------------------------------- 3.9/18.9 MB 213.1 kB/s eta 0:01:11\n",
      "   -------- ------------------------------- 4.2/18.9 MB 223.1 kB/s eta 0:01:06\n",
      "   --------- ------------------------------ 4.5/18.9 MB 233.8 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 4.5/18.9 MB 233.8 kB/s eta 0:01:02\n",
      "   --------- ------------------------------ 4.7/18.9 MB 244.8 kB/s eta 0:00:58\n",
      "   ---------- ----------------------------- 5.0/18.9 MB 255.7 kB/s eta 0:00:55\n",
      "   ----------- ---------------------------- 5.2/18.9 MB 267.0 kB/s eta 0:00:52\n",
      "   ------------ --------------------------- 5.8/18.9 MB 289.0 kB/s eta 0:00:46\n",
      "   ------------ --------------------------- 6.0/18.9 MB 300.1 kB/s eta 0:00:43\n",
      "   ------------- -------------------------- 6.3/18.9 MB 310.9 kB/s eta 0:00:41\n",
      "   ------------- -------------------------- 6.6/18.9 MB 321.4 kB/s eta 0:00:39\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 6.8/18.9 MB 329.7 kB/s eta 0:00:37\n",
      "   -------------- ------------------------- 7.1/18.9 MB 276.9 kB/s eta 0:00:43\n",
      "   -------------- ------------------------- 7.1/18.9 MB 276.9 kB/s eta 0:00:43\n",
      "   -------------- ------------------------- 7.1/18.9 MB 276.9 kB/s eta 0:00:43\n",
      "   -------------- ------------------------- 7.1/18.9 MB 276.9 kB/s eta 0:00:43\n",
      "   --------------- ------------------------ 7.3/18.9 MB 278.2 kB/s eta 0:00:42\n",
      "   --------------- ------------------------ 7.3/18.9 MB 278.2 kB/s eta 0:00:42\n",
      "   ---------------- ----------------------- 7.6/18.9 MB 283.7 kB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 7.6/18.9 MB 283.7 kB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 7.9/18.9 MB 290.1 kB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 8.1/18.9 MB 157.7 kB/s eta 0:01:09\n",
      "   ----------------- ---------------------- 8.1/18.9 MB 157.7 kB/s eta 0:01:09\n",
      "   ----------------- ---------------------- 8.1/18.9 MB 157.7 kB/s eta 0:01:09\n",
      "   ----------------- ---------------------- 8.1/18.9 MB 157.7 kB/s eta 0:01:09\n",
      "   ----------------- ---------------------- 8.1/18.9 MB 157.7 kB/s eta 0:01:09\n",
      "   ----------------- ---------------------- 8.4/18.9 MB 164.8 kB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 8.4/18.9 MB 164.8 kB/s eta 0:01:04\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.7/18.9 MB 171.4 kB/s eta 0:01:00\n",
      "   ------------------ --------------------- 8.9/18.9 MB 96.2 kB/s eta 0:01:44\n",
      "   ------------------ --------------------- 8.9/18.9 MB 96.2 kB/s eta 0:01:44\n",
      "   ------------------ --------------------- 8.9/18.9 MB 96.2 kB/s eta 0:01:44\n",
      "   ------------------ --------------------- 8.9/18.9 MB 96.2 kB/s eta 0:01:44\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.2/18.9 MB 82.8 kB/s eta 0:01:58\n",
      "   ------------------- -------------------- 9.4/18.9 MB 86.3 kB/s eta 0:01:50\n",
      "   ------------------- -------------------- 9.4/18.9 MB 86.3 kB/s eta 0:01:50\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   -------------------- ------------------- 9.7/18.9 MB 100.4 kB/s eta 0:01:32\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.0/18.9 MB 91.8 kB/s eta 0:01:38\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 10.2/18.9 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.5/18.9 MB 85.6 kB/s eta 0:01:39\n",
      "   ---------------------- ----------------- 10.7/18.9 MB 107.2 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 10.7/18.9 MB 107.2 kB/s eta 0:01:16\n",
      "   ---------------------- ----------------- 10.7/18.9 MB 107.2 kB/s eta 0:01:16\n",
      "   ----------------------- ---------------- 11.0/18.9 MB 145.9 kB/s eta 0:00:55\n",
      "   ----------------------- ---------------- 11.3/18.9 MB 182.9 kB/s eta 0:00:42\n",
      "   ------------------------ --------------- 11.5/18.9 MB 217.9 kB/s eta 0:00:34\n",
      "   ------------------------ --------------- 11.5/18.9 MB 217.9 kB/s eta 0:00:34\n",
      "   ------------------------ --------------- 11.8/18.9 MB 251.1 kB/s eta 0:00:29\n",
      "   ------------------------- -------------- 12.1/18.9 MB 283.0 kB/s eta 0:00:25\n",
      "   -------------------------- ------------- 12.3/18.9 MB 314.4 kB/s eta 0:00:21\n",
      "   --------------------------- ------------ 12.8/18.9 MB 372.0 kB/s eta 0:00:17\n",
      "   --------------------------- ------------ 13.1/18.9 MB 399.5 kB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 13.4/18.9 MB 426.5 kB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 13.6/18.9 MB 452.5 kB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 14.2/18.9 MB 503.3 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 14.7/18.9 MB 550.7 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 14.9/18.9 MB 574.2 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 15.5/18.9 MB 620.2 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 16.0/18.9 MB 665.1 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 16.5/18.9 MB 706.4 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 17.0/18.9 MB 748.2 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 17.6/18.9 MB 788.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 18.1/18.9 MB 827.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  18.9/18.9 MB 884.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.9/18.9 MB 880.5 kB/s eta 0:00:00\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.0 MB 2.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/12.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/12.0 MB 3.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.4/12.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.7/12.0 MB 635.8 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 4.2/12.0 MB 697.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 801.0 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 6.0/12.0 MB 941.6 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 7.3/12.0 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 11.0/12.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading torch-2.9.1-cp312-cp312-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.9/110.9 MB 13.9 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 7.1/110.9 MB 16.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.8/110.9 MB 20.1 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 19.9/110.9 MB 23.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 28.8/110.9 MB 27.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 39.3/110.9 MB 31.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 40.1/110.9 MB 29.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 42.2/110.9 MB 26.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 42.5/110.9 MB 23.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 43.0/110.9 MB 20.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 43.3/110.9 MB 19.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 43.5/110.9 MB 18.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 43.5/110.9 MB 18.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 43.8/110.9 MB 16.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 43.8/110.9 MB 16.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.0/110.9 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.3/110.9 MB 8.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 44.3/110.9 MB 8.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 44.6/110.9 MB 7.4 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 44.6/110.9 MB 7.4 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 44.8/110.9 MB 7.1 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 45.1/110.9 MB 6.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 45.1/110.9 MB 6.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 45.1/110.9 MB 6.8 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 45.4/110.9 MB 6.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 45.4/110.9 MB 6.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 45.6/110.9 MB 6.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 45.9/110.9 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 45.9/110.9 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 46.1/110.9 MB 5.6 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 46.4/110.9 MB 5.5 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 46.7/110.9 MB 5.4 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 46.9/110.9 MB 5.3 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 47.4/110.9 MB 5.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 47.7/110.9 MB 5.1 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 48.0/110.9 MB 5.0 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 48.5/110.9 MB 4.9 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 48.8/110.9 MB 4.9 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 49.3/110.9 MB 4.8 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 49.8/110.9 MB 4.8 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 50.3/110.9 MB 4.7 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 50.6/110.9 MB 4.7 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 51.1/110.9 MB 4.6 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 51.6/110.9 MB 4.6 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 52.4/110.9 MB 4.5 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 53.0/110.9 MB 4.5 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 53.5/110.9 MB 4.5 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 54.3/110.9 MB 4.5 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 54.8/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 55.6/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 56.1/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 56.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.7/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 57.9/110.9 MB 4.4 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 58.2/110.9 MB 3.5 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 58.2/110.9 MB 3.5 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 58.2/110.9 MB 3.5 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 58.2/110.9 MB 3.5 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.5/110.9 MB 3.3 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 58.7/110.9 MB 2.7 MB/s eta 0:00:20\n",
      "   --------------------- ------------------ 59.0/110.9 MB 2.4 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 59.0/110.9 MB 2.4 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 59.2/110.9 MB 2.4 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 59.5/110.9 MB 2.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 59.5/110.9 MB 2.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 60.0/110.9 MB 2.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 60.3/110.9 MB 2.3 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 61.1/110.9 MB 2.3 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 61.6/110.9 MB 2.3 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 62.4/110.9 MB 2.3 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 63.7/110.9 MB 2.4 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 65.3/110.9 MB 2.4 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 67.1/110.9 MB 2.5 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 69.7/110.9 MB 2.5 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 72.9/110.9 MB 2.6 MB/s eta 0:00:15\n",
      "   --------------------------- ------------ 77.6/110.9 MB 2.8 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 82.6/110.9 MB 2.9 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 86.5/110.9 MB 3.1 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 91.2/110.9 MB 3.2 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 94.1/110.9 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 95.7/110.9 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 97.3/110.9 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 98.8/110.9 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 100.4/110.9 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 102.0/110.9 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 103.5/110.9 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 104.3/110.9 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 105.6/110.9 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 106.7/110.9 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 108.0/110.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.1/110.9 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.4/110.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.9/110.9 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.9/110.9 MB 2.4 MB/s eta 0:00:00\n",
      "Using cached spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "Using cached scikit_learn-1.8.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "Downloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Downloading numpy-2.4.1-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.3 MB 6.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.3 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.7/12.3 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.3 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.6/12.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.3 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.0/12.3 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.3 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 5.8 MB/s eta 0:00:00\n",
      "Using cached preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Using cached regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.6 MB 5.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.4/38.6 MB 5.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.4/38.6 MB 5.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 4.7/38.6 MB 5.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.0/38.6 MB 5.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 7.3/38.6 MB 5.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 8.7/38.6 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 5.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.3/38.6 MB 6.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 12.6/38.6 MB 6.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.2/38.6 MB 6.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 15.5/38.6 MB 6.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.4/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.7/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 20.7/38.6 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 21.0/38.6 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 21.8/38.6 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 22.3/38.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.1/38.6 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 23.9/38.6 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.6/38.6 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 25.4/38.6 MB 5.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.2/38.6 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.0/38.6 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.8/38.6 MB 5.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.6/38.6 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.6/38.6 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.4/38.6 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.5/38.6 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.5/38.6 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.3/38.6 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.3/38.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.6 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 4.9 MB/s eta 0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Using cached wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Installing collected packages: mpmath, wrapt, wasabi, urllib3, typing_extensions, tqdm, threadpoolctl, sympy, spacy-loggers, spacy-legacy, setuptools, safetensors, regex, pyyaml, pathway, numpy, networkx, murmurhash, MarkupSafe, joblib, idna, fsspec, filelock, cymem, cloudpathlib, click, charset_normalizer, certifi, catalogue, annotated-types, typing-inspection, typer-slim, srsly, smart-open, scipy, requests, pydantic-core, preshed, jinja2, faiss-cpu, blis, torch, scikit-learn, pydantic, huggingface-hub, tokenizers, confection, weasel, transformers, thinc, spacy, sentence-transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\tahmi\\\\Documents\\\\Work\\\\HACKATHONS\\\\iitk_26\\\\venv\\\\Lib\\\\site-packages\\\\mpmath\\\\tests\\\\test_power.py'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathway\n",
      "  Using cached pathway-0.post1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.13.2-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-1.3.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Using cached numpy-2.4.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\tahmi\\documents\\work\\hackathons\\iitk_26\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Using cached weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tahmi\\documents\\work\\hackathons\\iitk_26\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Using cached pathway-0.post1-py3-none-any.whl (2.8 kB)\n",
      "Using cached sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Using cached faiss_cpu-1.13.2-cp312-cp312-win_amd64.whl (18.9 MB)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Downloading torch-2.9.1-cp312-cp312-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/110.9 MB 8.5 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 10.5/110.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 26.5/110.9 MB 54.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 43.3/110.9 MB 62.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 60.0/110.9 MB 67.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 71.8/110.9 MB 65.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 84.7/110.9 MB 64.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 97.8/110.9 MB 63.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 107.2/110.9 MB 61.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 60.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 110.9/110.9 MB 55.3 MB/s eta 0:00:00\n",
      "Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 6.6/14.2 MB 33.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.6/14.2 MB 32.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 30.8 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 5.2/8.0 MB 31.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.0 MB 21.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 13.8 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "Downloading fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading numpy-2.4.1-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 5.0/12.3 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.3 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 24.1 MB/s eta 0:00:00\n",
      "Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.4/38.6 MB 16.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.3/38.6 MB 17.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.5/38.6 MB 17.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 15.5/38.6 MB 18.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.7/38.6 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.9/38.6 MB 18.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.3/38.6 MB 18.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.5/38.6 MB 18.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.7/38.6 MB 19.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 18.4 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 654.8/654.8 kB 26.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 4.2/6.3 MB 21.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 17.8 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 14.9 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 20.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 18.9 MB/s eta 0:00:00\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 17.1 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Installing collected packages: mpmath, wrapt, wasabi, urllib3, typing_extensions, tqdm, threadpoolctl, sympy, spacy-loggers, spacy-legacy, setuptools, safetensors, regex, pyyaml, pathway, numpy, networkx, murmurhash, MarkupSafe, joblib, idna, fsspec, filelock, cymem, cloudpathlib, click, charset_normalizer, certifi, catalogue, annotated-types, typing-inspection, typer-slim, srsly, smart-open, scipy, requests, pydantic-core, preshed, jinja2, faiss-cpu, blis, torch, scikit-learn, pydantic, huggingface-hub, tokenizers, confection, weasel, transformers, thinc, spacy, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.3 annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 certifi-2026.1.4 charset_normalizer-3.4.4 click-8.3.1 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 faiss-cpu-1.13.2 filelock-3.20.3 fsspec-2026.1.0 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.3 mpmath-1.3.0 murmurhash-1.0.15 networkx-3.6.1 numpy-2.4.1 pathway-0.post1 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 sentence-transformers-5.2.0 setuptools-80.9.0 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 sympy-1.14.0 thinc-8.3.10 threadpoolctl-3.6.0 tokenizers-0.22.2 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 typer-slim-0.21.1 typing-inspection-0.4.2 typing_extensions-4.15.0 urllib3-2.6.3 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install pathway sentence-transformers faiss-cpu transformers torch spacy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f58599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 106, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 97, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 386, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 397, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
      "    return bool(self._sequence)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 174, in __bool__\n",
      "    return any(self)\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 162, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "                       ^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 53, in _iter_built\n",
      "    candidate = func()\n",
      "                ^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 187, in _make_candidate_from_link\n",
      "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 233, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 159, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 236, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 315, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 521, in prepare_linked_requirement\n",
      "    metadata_dist = self._fetch_metadata_only(req)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 373, in _fetch_metadata_only\n",
      "    return self._fetch_metadata_using_link_data_attr(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 393, in _fetch_metadata_using_link_data_attr\n",
      "    metadata_file = get_http_url(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 111, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 148, in __call__\n",
      "    for chunk in chunks:\n",
      "                 ^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "# pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acf681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7f603",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e37422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b0d6f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "nli = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"roberta-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32c7bf",
   "metadata": {},
   "source": [
    "# Load Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509df933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded books: ['In search of the castaways.txt', 'the count of monte cristo.txt']\n"
     ]
    }
   ],
   "source": [
    "# BOOK_DIR = \"Books\"\n",
    "\n",
    "# books = {}\n",
    "# for fname in os.listdir(BOOK_DIR):\n",
    "#     if fname.endswith(\".txt\"):\n",
    "#         with open(os.path.join(BOOK_DIR, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "#             books[fname] = f.read()\n",
    "\n",
    "# print(\"Loaded books:\", list(books.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f746121",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def chunk_text(text, max_tokens=900, overlap=200):\n",
    "# #     doc = nlp(text)\n",
    "# #     chunks, current, tokens = [], [], 0\n",
    "\n",
    "# #     for sent in doc.sents:\n",
    "# #         sent_len = len(sent)\n",
    "# #         if tokens + sent_len > max_tokens:\n",
    "# #             chunks.append(\" \".join(current))\n",
    "# #             current = current[-overlap:] if overlap < len(current) else current\n",
    "# #             tokens = sum(len(s.split()) for s in current)\n",
    "\n",
    "# #         current.append(sent.text)\n",
    "# #         tokens += sent_len\n",
    "\n",
    "# #     if current:\n",
    "# #         chunks.append(\" \".join(current))\n",
    "\n",
    "# #     return chunks\n",
    "\n",
    "# def chunk_text(text, max_chars=4000, overlap=500):\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     text_len = len(text)\n",
    "\n",
    "#     while start < text_len:\n",
    "#         end = start + max_chars\n",
    "#         chunk = text[start:end]\n",
    "#         chunks.append(chunk)\n",
    "#         start = end - overlap\n",
    "\n",
    "#     return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd46625",
   "metadata": {},
   "source": [
    "# Preprocess Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading preprocessed books from cache...\n",
      "\n",
      "Chunks per book:\n",
      "In search of the castaways.txt: 237\n",
      "The Count of Monte Cristo.txt: 757\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# CACHE_DIR = \"cache/v1\"\n",
    "# CHUNKS_PATH = os.path.join(CACHE_DIR, \"book_chunks.pkl\")\n",
    "# EMB_PATH = os.path.join(CACHE_DIR, \"book_embeddings.pkl\")\n",
    "\n",
    "# os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# def save_pickle(obj, path):\n",
    "#     with open(path, \"wb\") as f:\n",
    "#         pickle.dump(obj, f)\n",
    "\n",
    "# def load_pickle(path):\n",
    "#     with open(path, \"rb\") as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # Load from cache if available\n",
    "# # -------------------------------------------------\n",
    "# if os.path.exists(CHUNKS_PATH) and os.path.exists(EMB_PATH):\n",
    "#     print(\"==> Loading preprocessed books from cache...\")\n",
    "#     book_chunks = load_pickle(CHUNKS_PATH)\n",
    "#     book_embeddings = load_pickle(EMB_PATH)\n",
    "\n",
    "# else:\n",
    "#     print(\"==> Preprocessing books (chunking + embedding)...\")\n",
    "\n",
    "#     book_chunks = {}\n",
    "#     book_embeddings = {}\n",
    "\n",
    "#     for book_name, text in books.items():\n",
    "#         chunks = chunk_text(text)\n",
    "#         embeddings = embedder.encode(\n",
    "#             chunks,\n",
    "#             convert_to_numpy=True,\n",
    "#             show_progress_bar=True\n",
    "#         )\n",
    "\n",
    "#         book_chunks[book_name] = chunks\n",
    "#         book_embeddings[book_name] = embeddings\n",
    "\n",
    "#     save_pickle(book_chunks, CHUNKS_PATH)\n",
    "#     save_pickle(book_embeddings, EMB_PATH)\n",
    "\n",
    "#     print(\"==> Preprocessed data saved to cache.\")\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # Summary\n",
    "# # -------------------------------------------------\n",
    "# print(\"\\nChunks per book:\")\n",
    "# for k, v in book_chunks.items():\n",
    "#     print(f\"{k}: {len(v)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0696833",
   "metadata": {},
   "source": [
    "# Decompose Backstory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d8ebd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"data/raw/test.csv\"\n",
    "OUT_DIR = \"test_out/decomposed\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "def clean_text(t):\n",
    "    if pd.isna(t):\n",
    "        return \"\"\n",
    "    t = t.replace(\"\", \"'\").replace(\"\", \"\")\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea81e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_backstory(row):\n",
    "    claims = []\n",
    "    base_text = clean_text(row[\"content\"])\n",
    "    char = row[\"char\"]\n",
    "\n",
    "    sentences = sent_tokenize(base_text)\n",
    "\n",
    "    for sent in sentences:\n",
    "        # split compound sentences\n",
    "        parts = re.split(r\";| while | and \", sent)\n",
    "        for p in parts:\n",
    "            p = p.strip()\n",
    "            if len(p) < 15:\n",
    "                continue\n",
    "\n",
    "            # ensure subject presence\n",
    "            if char.lower() not in p.lower():\n",
    "                p = f\"{char} {p}\"\n",
    "\n",
    "            claims.append(p)\n",
    "\n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0ed22d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 150 atomic claims.\n"
     ]
    }
   ],
   "source": [
    "claims_out = []\n",
    "index_out = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    claims = decompose_backstory(row)\n",
    "\n",
    "    for i, claim in enumerate(claims):\n",
    "        cid = f\"{row['id']}_{i}\"\n",
    "\n",
    "        claims_out.append({\n",
    "            \"claim_id\": cid,\n",
    "            \"row_id\": int(row[\"id\"]),\n",
    "            \"book_name\": row[\"book_name\"],\n",
    "            \"character\": row[\"char\"],\n",
    "            \"claim_text\": claim,\n",
    "            \"source\": \"content\"\n",
    "            # \"label\": row[\"label\"]\n",
    "        })\n",
    "\n",
    "        index_out.append({\n",
    "            \"claim_id\": cid,\n",
    "            \"row_id\": int(row[\"id\"])\n",
    "        })\n",
    "\n",
    "# save\n",
    "with open(f\"{OUT_DIR}/claims.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in claims_out:\n",
    "        f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(f\"{OUT_DIR}/index.json\", \"w\") as f:\n",
    "    json.dump(index_out, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(claims_out)} atomic claims.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fcbf4",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "claim  embed  retrieve top-k relevant chunks\n",
    "                    (while avoiding redundancy)\n",
    "\n",
    "MMR(d) =   sim(q, d)  (1  )  max sim(d, d_selected)\n",
    "\n",
    "  0.60.8 (relevance vs diversity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a095fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading claims...\n",
      "Loading book chunks...\n",
      "Loading book embeddings...\n",
      "\n",
      "Validating book alignment...\n",
      " Book alignment OK\n",
      "(237, 768)\n",
      "\n",
      "Retrieving evidence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 150/150 [00:08<00:00, 16.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done. Wrote evidence for 150 claims.\n",
      "Output saved to: test_out/retrieval/claim_evidence.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CLAIMS_PATH = \"test_out/decomposed/claims.jsonl\"\n",
    "BOOK_CHUNKS_PATH = \"data/books/book_chunks.pkl\"\n",
    "BOOK_EMBEDS_PATH = \"data/books/book_embeddings.pkl\"\n",
    "OUT_DIR = \"test_out/retrieval\"\n",
    "OUT_PATH = f\"{OUT_DIR}/claim_evidence.jsonl\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def normalize_book_name(name: str) -> str:\n",
    "    return re.sub(r\"\\.txt$\", \"\", name.lower().strip())\n",
    "\n",
    "print(\"Loading claims...\")\n",
    "claims = [json.loads(l) for l in open(CLAIMS_PATH, encoding=\"utf-8\")]\n",
    "assert len(claims) > 0, \"claims.jsonl is empty\"\n",
    "\n",
    "print(\"Loading book chunks...\")\n",
    "with open(BOOK_CHUNKS_PATH, \"rb\") as f:\n",
    "    raw_chunks = pickle.load(f)\n",
    "\n",
    "print(\"Loading book embeddings...\")\n",
    "with open(BOOK_EMBEDS_PATH, \"rb\") as f:\n",
    "    raw_embeddings = pickle.load(f)\n",
    "\n",
    "# normalize book keys\n",
    "book_chunks = {\n",
    "    normalize_book_name(k): v\n",
    "    for k, v in raw_chunks.items()\n",
    "}\n",
    "\n",
    "book_embeddings = {\n",
    "    normalize_book_name(k): v\n",
    "    for k, v in raw_embeddings.items()\n",
    "}\n",
    "\n",
    "# Validation\n",
    "\n",
    "\n",
    "print(\"\\nValidating book alignment...\")\n",
    "\n",
    "for k in book_chunks:\n",
    "    assert k in book_embeddings, f\"Missing embeddings for book: {k}\"\n",
    "    assert len(book_chunks[k]) == book_embeddings[k].shape[0], \\\n",
    "        f\"Chunk/embedding mismatch in {k}\"\n",
    "\n",
    "missing_books = {\n",
    "    c[\"book_name\"]\n",
    "    for c in claims\n",
    "    if normalize_book_name(c[\"book_name\"]) not in book_embeddings\n",
    "}\n",
    "\n",
    "assert not missing_books, f\"Claims reference missing books: {missing_books}\"\n",
    "\n",
    "print(\" Book alignment OK\")\n",
    "\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "print(book_embeddings[next(iter(book_embeddings))].shape)\n",
    "\n",
    "# =============================================\n",
    "# mmr imp.\n",
    "\n",
    "def mmr(query_emb, doc_embs, top_k=5, lambda_param=0.7):\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance selection.\n",
    "    \"\"\"\n",
    "    query_emb = query_emb.reshape(1, -1)\n",
    "    sim_q = cosine_similarity(query_emb, doc_embs)[0]\n",
    "    sim_dd = cosine_similarity(doc_embs, doc_embs)\n",
    "\n",
    "    selected = []\n",
    "    candidates = list(range(len(doc_embs)))\n",
    "\n",
    "    for _ in range(min(top_k, len(candidates))):\n",
    "        best_score, best_idx = -1e9, None\n",
    "\n",
    "        for idx in candidates:\n",
    "            relevance = sim_q[idx]\n",
    "            diversity = max(\n",
    "                sim_dd[idx][s] for s in selected\n",
    "            ) if selected else 0.0\n",
    "\n",
    "            score = lambda_param * relevance - (1 - lambda_param) * diversity\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score, best_idx = score, idx\n",
    "\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "\n",
    "    return selected\n",
    "\n",
    "# =========================================================\n",
    "# Retrieval\n",
    "\n",
    "print(\"\\nRetrieving evidence...\")\n",
    "\n",
    "written = 0\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for claim in tqdm(claims):\n",
    "        book = normalize_book_name(claim[\"book_name\"])\n",
    "\n",
    "        chunks = book_chunks[book]\n",
    "        embeds = book_embeddings[book]\n",
    "\n",
    "        q_emb = embedder.encode(\n",
    "            claim[\"claim_text\"],\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        selected_ids = mmr(\n",
    "            q_emb,\n",
    "            embeds,\n",
    "            top_k=5,\n",
    "            lambda_param=0.7\n",
    "        )\n",
    "\n",
    "        evidence = [{\n",
    "            \"chunk_id\": idx,\n",
    "            \"text\": chunks[idx]\n",
    "        } for idx in selected_ids]\n",
    "\n",
    "        fout.write(json.dumps({\n",
    "            \"claim_id\": claim[\"claim_id\"],\n",
    "            \"row_id\": claim[\"row_id\"],\n",
    "            \"book_name\": claim[\"book_name\"],\n",
    "            \"character\": claim[\"character\"],\n",
    "            \"claim_text\": claim[\"claim_text\"],\n",
    "            # \"label\": claim[\"label\"],\n",
    "            \"evidence\": evidence\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        written += 1\n",
    "\n",
    "assert written > 0, \"No evidence written  pipeline failure\"\n",
    "\n",
    "print(f\"\\n Done. Wrote evidence for {written} claims.\")\n",
    "print(f\"Output saved to: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14044e66",
   "metadata": {},
   "source": [
    "# NLI CLassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143f847",
   "metadata": {},
   "source": [
    "### Generate weak NLI Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "IN_PATH = \"data/retrieval/claim_evidence.jsonl\"\n",
    "OUT_DIR = \"data/nli\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "NEGATIONS = {\"not\", \"never\", \"no\", \"none\", \"without\", \"denied\", \"didn't\", \"wasn't\"}\n",
    "\n",
    "def tokenize(t):\n",
    "    return set(re.findall(r\"\\b\\w+\\b\", t.lower()))\n",
    "\n",
    "def label_pair(claim, evidence, global_label, character):\n",
    "    c_tok = tokenize(claim)\n",
    "    e_tok = tokenize(evidence)\n",
    "\n",
    "    overlap = len(c_tok & e_tok)\n",
    "    has_neg = any(n in e_tok for n in NEGATIONS)\n",
    "    mentions_char = character.lower() in evidence.lower()\n",
    "\n",
    "    if global_label == \"consistent\":\n",
    "        if overlap >= 3 and mentions_char:\n",
    "            return \"SUPPORT\"\n",
    "        else:\n",
    "            return \"NEUTRAL\"\n",
    "\n",
    "    if global_label == \"contradict\":\n",
    "        if has_neg and overlap >= 2:\n",
    "            return \"CONTRADICT\"\n",
    "        else:\n",
    "            return \"NEUTRAL\"\n",
    "\n",
    "    return \"NEUTRAL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cafbd6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "label_pair() missing 1 required positional argument: 'character'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m ex = json.loads(line)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ev \u001b[38;5;129;01min\u001b[39;00m ex[\u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     lbl = \u001b[43mlabel_pair\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaim_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mev\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ex[\"label\"],\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcharacter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     pairs.append({\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclaim_id\u001b[39m\u001b[33m\"\u001b[39m: ex[\u001b[33m\"\u001b[39m\u001b[33mclaim_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m: ex[\u001b[33m\"\u001b[39m\u001b[33mclaim_text\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m: ev[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: lbl\n\u001b[32m     21\u001b[39m     })\n\u001b[32m     23\u001b[39m     label_counts[lbl] += \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: label_pair() missing 1 required positional argument: 'character'"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "label_counts = Counter()\n",
    "\n",
    "with open(IN_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "\n",
    "        for ev in ex[\"evidence\"]:\n",
    "            lbl = label_pair(\n",
    "                ex[\"claim_text\"],\n",
    "                ev[\"text\"],\n",
    "                # ex[\"label\"],\n",
    "                ex[\"character\"]\n",
    "            )\n",
    "\n",
    "            pairs.append({\n",
    "                \"claim_id\": ex[\"claim_id\"],\n",
    "                \"claim\": ex[\"claim_text\"],\n",
    "                \"evidence\": ev[\"text\"],\n",
    "                \"label\": lbl\n",
    "            })\n",
    "\n",
    "            label_counts[lbl] += 1\n",
    "\n",
    "print(\"Label distribution:\", label_counts)\n",
    "\n",
    "with open(f\"{OUT_DIR}/nli_pairs.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in pairs:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80f3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(pairs)\n",
    "\n",
    "n = len(pairs)\n",
    "train, dev, test = (\n",
    "    pairs[: int(0.8 * n)],\n",
    "    pairs[int(0.8 * n): int(0.9 * n)],\n",
    "    pairs[int(0.9 * n):],\n",
    ")\n",
    "\n",
    "def dump(name, data):\n",
    "    with open(f\"{OUT_DIR}/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for x in data:\n",
    "            f.write(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "dump(\"nli_train\", train)\n",
    "dump(\"nli_dev\", dev)\n",
    "dump(\"nli_test\", test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f33aaf",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111d0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "LABEL_MAP = {\"SUPPORT\": 0, \"NEUTRAL\": 1, \"CONTRADICT\": 2}\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=256):\n",
    "        self.data = [json.loads(l) for l in open(path, encoding=\"utf-8\")]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "\n",
    "        text = ex[\"claim\"] + \" [SEP] \" + ex[\"evidence\"]\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(LABEL_MAP[ex[\"label\"]])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd819656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\tahmi\\AppData\\Local\\Temp\\ipykernel_3024\\1935616625.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 40:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.044169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.973174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.043400</td>\n",
       "      <td>0.934112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\tahmi\\Documents\\Work\\HACKATHONS\\iitk_26\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Best NLI model saved to: models/nli/best_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "OUTPUT_DIR = \"models/nli\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=False        \n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = NLIDataset(\"data/nli/nli_train.jsonl\", tokenizer)\n",
    "dev_ds   = NLIDataset(\"data/nli/nli_dev.jsonl\", tokenizer)\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,        \n",
    "    eval_strategy=\"epoch\",         \n",
    "    save_strategy=\"epoch\",          \n",
    "    save_total_limit=3,             \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"                \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=dev_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=\"models/nli/checkpoint-1000\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "BEST_DIR = f\"{OUTPUT_DIR}/best_model\"\n",
    "\n",
    "trainer.save_model(BEST_DIR)\n",
    "tokenizer.save_pretrained(BEST_DIR)\n",
    "\n",
    "print(f\"==> Best NLI model saved to: {BEST_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[Any] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n"
     ]
    }
   ],
   "source": [
    "# import inspect\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# print(inspect.signature(TrainingArguments.__init__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1b67f",
   "metadata": {},
   "source": [
    "# Temporal and Casual Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a8dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"models/nli/best_model\"\n",
    "CLAIM_EVIDENCE = \"test_out/retrieval/claim_evidence.jsonl\"\n",
    "CLAIMS_PATH = \"test_out/decomposed/claims.jsonl\"\n",
    "OUT_FINAL = \"test_out/final_predictions.jsonl\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "LABELS = [\"SUPPORT\", \"NEUTRAL\", \"CONTRADICT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ff07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_by_claim = defaultdict(list)\n",
    "\n",
    "with open(CLAIM_EVIDENCE, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "\n",
    "        for ev in ex[\"evidence\"]:\n",
    "            text = ex[\"claim_text\"] + \" [SEP] \" + ev[\"text\"]\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=-1)[0].tolist()\n",
    "\n",
    "            nli_by_claim[ex[\"claim_id\"]].append({\n",
    "                \"chunk_id\": ev[\"chunk_id\"],\n",
    "                \"probs\": dict(zip(LABELS, probs)),\n",
    "                \"text\": ev[\"text\"],\n",
    "                \"row_id\": ex[\"row_id\"],\n",
    "                \"character\": ex[\"character\"]\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e46f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLI_OUT = \"test_out/nli/nli_predictions.jsonl\"\n",
    "\n",
    "with open(NLI_OUT, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for claim_id, preds in nli_by_claim.items():\n",
    "        for p in preds:\n",
    "            fout.write(json.dumps({\n",
    "                \"claim_id\": claim_id,\n",
    "                **p\n",
    "            }) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "950f3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORTANT_VERBS = {\n",
    "    \"read\", \"write\", \"learn\", \"teach\", \"know\",\n",
    "    \"escape\", \"die\", \"kill\", \"marry\",\n",
    "    \"imprison\", \"free\", \"fight\", \"travel\"\n",
    "}\n",
    "\n",
    "STOP_SUBJECTS = {\"he\", \"she\", \"they\", \"it\", \"who\", \"which\"}\n",
    "\n",
    "def extract_events(text, time_anchor):\n",
    "    doc = nlp(text)\n",
    "    events = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for tok in sent:\n",
    "            if tok.pos_ == \"VERB\":\n",
    "                lemma = tok.lemma_.lower()\n",
    "\n",
    "                if lemma not in IMPORTANT_VERBS:\n",
    "                    continue\n",
    "\n",
    "                subj = next(\n",
    "                    (c.text for c in tok.children if c.dep_ == \"nsubj\"),\n",
    "                    None\n",
    "                )\n",
    "\n",
    "                if not subj:\n",
    "                    continue\n",
    "\n",
    "                subj = subj.lower()\n",
    "\n",
    "                if subj in STOP_SUBJECTS:\n",
    "                    continue\n",
    "\n",
    "                events.append({\n",
    "                    \"character\": subj,\n",
    "                    \"event\": lemma,\n",
    "                    \"time\": time_anchor\n",
    "                })\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def claim_to_constraints(claim, character):\n",
    "    \"\"\"\n",
    "    Convert a natural-language claim into\n",
    "    temporal / causal constraints.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "        - character\n",
    "        - forbidden_events\n",
    "        - allowed_after\n",
    "        - strength   (soft | hard)\n",
    "    \"\"\"\n",
    "    c = claim.lower()\n",
    "    character = character.lower()\n",
    "\n",
    "    # -----------------------------\n",
    "    # ABILITY / KNOWLEDGE CLAIMS\n",
    "    # -----------------------------\n",
    "    if any(k in c for k in [\"illiterate\", \"could not read\", \"unable to read\"]):\n",
    "        return {\n",
    "            \"character\": character,\n",
    "            \"forbidden_events\": [\"read\", \"write\"],\n",
    "            \"allowed_after\": [\"learn\", \"teach\", \"study\"],\n",
    "            \"strength\": \"soft\"\n",
    "        }\n",
    "\n",
    "    if any(k in c for k in [\"educated\", \"well-read\", \"scholar\"]):\n",
    "        return {\n",
    "            \"character\": character,\n",
    "            \"forbidden_events\": [],\n",
    "            \"allowed_after\": [\"learn\", \"teach\", \"study\"],\n",
    "            \"strength\": \"soft\"\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # MOVEMENT / ESCAPE CLAIMS\n",
    "    # -----------------------------\n",
    "    if any(k in c for k in [\"never escaped\", \"did not escape\", \"failed to escape\"]):\n",
    "        return {\n",
    "            \"character\": character,\n",
    "            \"forbidden_events\": [\"escape\", \"travel\"],\n",
    "            \"allowed_after\": [],\n",
    "            \"strength\": \"hard\"\n",
    "        }\n",
    "\n",
    "    if any(k in c for k in [\"escaped\", \"fled\", \"broke free\"]):\n",
    "        return {\n",
    "            \"character\": character,\n",
    "            \"forbidden_events\": [],\n",
    "            \"allowed_after\": [\"escape\"],\n",
    "            \"strength\": \"soft\"\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # LIFE / DEATH CLAIMS\n",
    "    # -----------------------------\n",
    "    if any(k in c for k in [\"died\", \"was killed\", \"perished\"]):\n",
    "        return {\n",
    "            \"character\": character,\n",
    "            \"forbidden_events\": [\n",
    "                \"travel\", \"fight\", \"marry\", \"teach\", \"escape\"\n",
    "            ],\n",
    "            \"allowed_after\": [],\n",
    "            \"strength\": \"hard\"\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # RELATIONSHIP CLAIMS\n",
    "    # -----------------------------\n",
    "    if any(k in c for k in [\"never married\", \"remained single\"]):\n",
    "        return {\n",
    "            \"character\": character,\n",
    "            \"forbidden_events\": [\"marry\"],\n",
    "            \"allowed_after\": [],\n",
    "            \"strength\": \"hard\"\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # DEFAULT\n",
    "    # -----------------------------\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "800edc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_violation_score(claim_struct, timeline):\n",
    "    \"\"\"\n",
    "    Returns a soft temporal violation penalty in [0, 1].\n",
    "\n",
    "    - Scales with number of violations\n",
    "    - Normalized by timeline size\n",
    "    - Respects hard vs soft constraints\n",
    "    - Stronger penalty for earlier violations\n",
    "    \"\"\"\n",
    "\n",
    "    char = claim_struct[\"character\"]\n",
    "    forbidden = claim_struct.get(\"forbidden_events\", [])\n",
    "    allowed_after = claim_struct.get(\"allowed_after\", [])\n",
    "    strength = claim_struct.get(\"strength\", \"soft\")\n",
    "\n",
    "    events = timeline.get(char, [])\n",
    "    if not events:\n",
    "        return 0.0\n",
    "\n",
    "    # sort by time (early events matter more)\n",
    "    events = sorted(events, key=lambda x: x[\"time\"])\n",
    "\n",
    "    violation_score = 0.0\n",
    "    total_weight = 0.0\n",
    "\n",
    "    for idx, e in enumerate(events):\n",
    "        if e[\"event\"] not in forbidden:\n",
    "            continue\n",
    "\n",
    "        # earlier violations are more severe\n",
    "        time_weight = 1.0 - (idx / len(events))\n",
    "\n",
    "        explained = any(\n",
    "            ev[\"event\"] in allowed_after and ev[\"time\"] < e[\"time\"]\n",
    "            for ev in events\n",
    "        )\n",
    "\n",
    "        if not explained:\n",
    "            violation_score += time_weight\n",
    "            total_weight += time_weight\n",
    "\n",
    "    if total_weight == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # normalize to [0, 1]\n",
    "    penalty = violation_score / total_weight\n",
    "\n",
    "    # hard constraints are more serious\n",
    "    if strength == \"hard\":\n",
    "        penalty *= 1.0\n",
    "    else:\n",
    "        penalty *= 0.5\n",
    "\n",
    "    return min(penalty, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30a2da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_row(decisions):\n",
    "    \"\"\"\n",
    "    Balanced row-level aggregation.\n",
    "    Consistency must be supported.\n",
    "    Contradiction must be explicit.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(decisions)\n",
    "\n",
    "    c_contra = decisions.count(\"CONTRADICT\")\n",
    "    c_supp   = decisions.count(\"SUPPORT\")\n",
    "    c_neu    = decisions.count(\"NEUTRAL\")\n",
    "\n",
    "    # explicit contradiction dominates\n",
    "    if c_contra >= 1:\n",
    "        return \"contradict\"\n",
    "\n",
    "    # support dominates\n",
    "    if c_supp >= max(2, n // 2 + 1):\n",
    "        return \"consistent\"\n",
    "\n",
    "    # Ambiguous cases: lean to consistent\n",
    "    # (neutral  contradiction)\n",
    "    return \"consistent\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decide_claim(nli_preds, claim_struct, timeline):\n",
    "    \"\"\"\n",
    "    Calibrated claim-level decision.\n",
    "    Designed for weak NLI + literary text.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(nli_preds)\n",
    "\n",
    "    mean_contra = sum(p[\"probs\"][\"CONTRADICT\"] for p in nli_preds) / n\n",
    "    mean_supp   = sum(p[\"probs\"][\"SUPPORT\"]    for p in nli_preds) / n\n",
    "    mean_neu    = sum(p[\"probs\"][\"NEUTRAL\"]    for p in nli_preds) / n\n",
    "\n",
    "    max_contra = max(p[\"probs\"][\"CONTRADICT\"] for p in nli_preds)\n",
    "    max_supp   = max(p[\"probs\"][\"SUPPORT\"]    for p in nli_preds)\n",
    "\n",
    "    strong_contra = sum(p[\"probs\"][\"CONTRADICT\"] > 0.6 for p in nli_preds)\n",
    "    strong_supp   = sum(p[\"probs\"][\"SUPPORT\"]    > 0.6 for p in nli_preds)\n",
    "\n",
    "    # ---- base semantic score ----\n",
    "    score = (\n",
    "        0.35 * (mean_contra - mean_supp) +\n",
    "        0.35 * (max_contra  - max_supp) +\n",
    "        0.30 * (strong_contra - strong_supp)\n",
    "    )\n",
    "\n",
    "    # ---- softer neutral dampening ----\n",
    "    score *= (1.0 - 0.7 * mean_neu)\n",
    "\n",
    "    # ---- temporal reasoning ----\n",
    "    if claim_struct:\n",
    "        temporal_penalty = temporal_violation_score(claim_struct, timeline)\n",
    "\n",
    "        # temporal logic can tip the balance\n",
    "        score += 0.3 * temporal_penalty\n",
    "\n",
    "    # ---- decision thresholds (ASYMMETRIC) ----\n",
    "    if score > 0.05:\n",
    "        return \"CONTRADICT\"\n",
    "\n",
    "    if score < -0.10:\n",
    "        return \"SUPPORT\"\n",
    "\n",
    "    return \"NEUTRAL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4e7c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading NLI predictions: 750it [00:00, 26108.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded NLI predictions for 150 claims\n",
      " Loading cached timeline...\n",
      " Loaded timelines for 107 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating claims: 150it [00:00, 29939.35it/s]\n",
      "Writing final predictions: 100%|| 60/60 [00:00<00:00, 48941.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Final predictions written to: test_out/final_predictions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "\n",
    "CLAIM_EVIDENCE = \"test_out/retrieval/claim_evidence.jsonl\"\n",
    "CLAIMS_PATH = \"test_out/decomposed/claims.jsonl\"\n",
    "NLI_PRED_PATH = \"test_out/nli/nli_predictions.jsonl\"\n",
    "OUT_FINAL = \"test_out/final_predictions.jsonl\"\n",
    "TIMELINE_CACHE = \"data/cache/timeline.json\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. LOAD NLI PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "nli_by_claim = defaultdict(list)\n",
    "\n",
    "with open(NLI_PRED_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Loading NLI predictions\"):\n",
    "        p = json.loads(line)\n",
    "        nli_by_claim[p[\"claim_id\"]].append(p)\n",
    "\n",
    "print(f\" Loaded NLI predictions for {len(nli_by_claim)} claims\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. BUILD EVENT TIMELINES\n",
    "# ============================================================\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "timeline = defaultdict(list)\n",
    "\n",
    "def normalize_name(name):\n",
    "    return name.lower().strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load cached timeline if available\n",
    "# ------------------------------------------------------------\n",
    "if os.path.exists(TIMELINE_CACHE):\n",
    "    print(\" Loading cached timeline...\")\n",
    "\n",
    "    with open(TIMELINE_CACHE, encoding=\"utf-8\") as f:\n",
    "        timeline_data = json.load(f)\n",
    "\n",
    "    for char, events in timeline_data.items():\n",
    "        timeline[char].extend(events)\n",
    "\n",
    "    print(f\" Loaded timelines for {len(timeline)} characters\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Otherwise, build timeline and cache it\n",
    "# ------------------------------------------------------------\n",
    "else:\n",
    "    print(\" Building timeline from evidence...\")\n",
    "\n",
    "    with open(CLAIM_EVIDENCE, encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Building timelines\"):\n",
    "            ex = json.loads(line)\n",
    "            time_anchor = ex[\"row_id\"]  # coarse temporal ordering\n",
    "\n",
    "            for ev in ex[\"evidence\"]:\n",
    "                events = extract_events(ev[\"text\"], time_anchor)\n",
    "\n",
    "                for e in events:\n",
    "                    e[\"character\"] = normalize_name(e[\"character\"])\n",
    "                    timeline[e[\"character\"]].append(e)\n",
    "\n",
    "    # Save as a normal dict (JSON-safe)\n",
    "    with open(TIMELINE_CACHE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dict(timeline), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\" Built & cached timelines for {len(timeline)} characters\")\n",
    "    \n",
    "    \n",
    "# ============================================================\n",
    "# 3. CLAIM-LEVEL AGGREGATION\n",
    "# ============================================================\n",
    "\n",
    "row_votes = defaultdict(list)\n",
    "\n",
    "with open(CLAIMS_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Aggregating claims\"):\n",
    "        c = json.loads(line)\n",
    "\n",
    "        claim_id = c[\"claim_id\"]\n",
    "        row_id = c[\"row_id\"]\n",
    "        character = normalize_name(c[\"character\"])\n",
    "\n",
    "        preds = nli_by_claim.get(claim_id)\n",
    "        if not preds:\n",
    "            continue\n",
    "\n",
    "        struct = claim_to_constraints(c[\"claim_text\"], character)\n",
    "        decision = decide_claim(preds, struct, timeline)\n",
    "\n",
    "        row_votes[row_id].append(decision)\n",
    "\n",
    "# ============================================================\n",
    "# 4. ROW-LEVEL FINAL DECISION (UPDATED)\n",
    "# ============================================================\n",
    "\n",
    "with open(OUT_FINAL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row_id, decisions in tqdm(row_votes.items(), desc=\"Writing final predictions\"):\n",
    "        final = decide_row(decisions)\n",
    "\n",
    "        f.write(json.dumps({\n",
    "            \"row_id\": row_id,\n",
    "            \"prediction\": final,\n",
    "            \"claim_decisions\": decisions\n",
    "        }) + \"\\n\")\n",
    "\n",
    "print(f\"\\n==> Final predictions written to: {OUT_FINAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c847552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Claim-level distribution:\n",
      "Counter({'NEUTRAL': 118, 'CONTRADICT': 32})\n",
      "\n",
      "Row-level distribution:\n",
      "Counter({'consistent': 38, 'contradict': 22})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"\\nClaim-level distribution:\")\n",
    "print(Counter(d for decisions in row_votes.values() for d in decisions))\n",
    "\n",
    "print(\"\\nRow-level distribution:\")\n",
    "print(Counter(decide_row(decisions) for decisions in row_votes.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090ce2",
   "metadata": {},
   "source": [
    "# Rational in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e81f7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_rationale(\n",
    "    row_id,\n",
    "    final_label,\n",
    "    claim_decisions,\n",
    "    nli_preds_by_row,\n",
    "    timeline=None,\n",
    "    max_snippet_words=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive evidence-based rationale suitable for Track B.\n",
    "    \"\"\"\n",
    "\n",
    "    preds = nli_preds_by_row.get(row_id, [])\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # CONTRADICTION CASE\n",
    "    # ----------------------------------------\n",
    "    if final_label == \"contradict\":\n",
    "        # find strongest contradiction\n",
    "        strongest = None\n",
    "        for p in preds:\n",
    "            if p[\"probs\"][\"CONTRADICT\"] > 0.5:\n",
    "                strongest = p\n",
    "                break\n",
    "\n",
    "        if strongest:\n",
    "            text = strongest[\"text\"]\n",
    "            snippet = \" \".join(text.split()[:max_snippet_words]) + \"...\"\n",
    "\n",
    "            return (\n",
    "                \"The proposed backstory was evaluated against multiple passages in the novel. \"\n",
    "                \"Retrieved evidence reveals later actions or events that conflict with the claimed \"\n",
    "                f\"background, for example: \\\"{snippet}\\\"\"\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            \"The backstory claims were tested against the novel text, and retrieved evidence \"\n",
    "            \"indicates inconsistencies with events described later in the narrative.\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # CONSISTENT CASE\n",
    "    # ----------------------------------------\n",
    "    if all(d == \"NEUTRAL\" for d in claim_decisions):\n",
    "        return (\n",
    "            \"The backstory claims were systematically evaluated against relevant passages \"\n",
    "            \"from the novel. No retrieved evidence was found that contradicts the stated \"\n",
    "            \"background, indicating consistency with the narrative.\"\n",
    "        )\n",
    "\n",
    "    # Partial support case\n",
    "    for p in preds:\n",
    "        if p[\"probs\"][\"SUPPORT\"] > 0.5:\n",
    "            text = p[\"text\"]\n",
    "            snippet = \" \".join(text.split()[:max_snippet_words]) + \"...\"\n",
    "\n",
    "            return (\n",
    "                \"The backstory was assessed using retrieved evidence from the novel. \"\n",
    "                \"Relevant passages support the claimed background, such as: \"\n",
    "                f\"\\\"{snippet}\\\"\"\n",
    "            )\n",
    "\n",
    "    return (\n",
    "        \"The backstory was examined against the primary text, and the available evidence \"\n",
    "        \"does not reveal contradictions with the narrative events.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "448a6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded NLI predictions for 60 stories\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "NLI_PATH = \"test_out/nli/nli_predictions.jsonl\"\n",
    "\n",
    "nli_preds_by_row = defaultdict(list)\n",
    "\n",
    "with open(NLI_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        p = json.loads(line)\n",
    "        nli_preds_by_row[p[\"row_id\"]].append(p)\n",
    "\n",
    "print(f\" Loaded NLI predictions for {len(nli_preds_by_row)} stories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba323750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Submission with rationale written to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "FINAL_JSON = \"test_out/final_predictions.jsonl\"\n",
    "OUT_CSV = \"submission.csv\"\n",
    "\n",
    "with open(FINAL_JSON, encoding=\"utf-8\") as fin, \\\n",
    "     open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow([\"Story ID\", \"Prediction\", \"Rationale\"])\n",
    "\n",
    "    for line in fin:\n",
    "        ex = json.loads(line)\n",
    "\n",
    "        row_id = ex[\"row_id\"]\n",
    "        final_label = ex[\"prediction\"]            # \"consistent\" / \"contradict\"\n",
    "        claim_decisions = ex[\"claim_decisions\"]\n",
    "\n",
    "        # map to required label\n",
    "        prediction = 1 if final_label == \"consistent\" else 0\n",
    "\n",
    "        #  USE YOUR FUNCTION HERE\n",
    "        rationale = generate_comprehensive_rationale(\n",
    "            row_id=row_id,\n",
    "            final_label=final_label,\n",
    "            claim_decisions=claim_decisions,\n",
    "            nli_preds_by_row=nli_preds_by_row,\n",
    "            timeline=timeline  # optional, can pass None\n",
    "        )\n",
    "\n",
    "        writer.writerow([row_id, prediction, rationale])\n",
    "\n",
    "print(f\" Submission with rationale written to: {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fccb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
